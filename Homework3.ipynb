{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 1a: Parse through data\n",
    "amazon = pd.read_table('/Users/leighajarett/Downloads/sent/amazon_cells_labelled.txt', sep ='\\t', header = None)\n",
    "yelp = pd.read_table('/Users/leighajarett/Downloads/sent/yelp_labelled.txt', sep ='\\t', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#imdb = pd.read_excel('/Users/leighajarett/Downloads/sent/imdb_labelled1.xlsx', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#imdb.to_csv('/Users/leighajarett/Desktop/imdb.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imdb = pd.read_csv('/Users/leighajarett/Desktop/imdb.csv', skiprows = 1, header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    500\n",
       "0    500\n",
       "Name: 1, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# are the labels balanced?\n",
    "amazon[1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    500\n",
       "0    500\n",
       "Name: 1, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb[1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    500\n",
       "0    500\n",
       "Name: 1, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp[1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#1b: Preprocessing \n",
    "\n",
    "#make lowercase\n",
    "amazon = amazon.apply(lambda x: x.astype(str).str.lower())\n",
    "imdb = imdb.apply(lambda x: x.astype(str).str.lower())\n",
    "yelp = yelp.apply(lambda x: x.astype(str).str.lower())\n",
    "\n",
    "# ref: http://stackoverflow.com/questions/39512002/convert-whole-dataframe-from-lower-case-to-upper-case-with-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "def remove_punc(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation,'')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "strings_a = amazon[0].apply(remove_punc)\n",
    "labels_a = amazon[1]\n",
    "amazon = pd.concat([strings_a, labels_a], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "strings_i = imdb[0].apply(remove_punc)\n",
    "labels_i = imdb[1]\n",
    "imdb = pd.concat([strings_i, labels_i], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "strings_y = yelp[0].apply(remove_punc)\n",
    "labels_y = yelp[1]\n",
    "yelp = pd.concat([strings_y, labels_y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ref: http://stackoverflow.com/questions/18062135/combining-two-series-into-a-dataframe-in-pandas\n",
    "#ref: http://chrisalbon.com/python/pandas_apply_operations_to_dataframes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenize and get rid of stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amazon[0] = amazon.apply(lambda row: nltk.word_tokenize(row[0]), axis=1)\n",
    "amazon[0]=amazon[0].apply(lambda x: [item for item in x if item not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imdb[0] = imdb.apply(lambda row: nltk.word_tokenize(row[0]), axis=1)\n",
    "imdb[0]=imdb[0].apply(lambda x: [item for item in x if item not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yelp[0] = yelp.apply(lambda row: nltk.word_tokenize(row[0]), axis=1)\n",
    "yelp[0]=yelp[0].apply(lambda x: [item for item in x if item not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lemmatize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lmt = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "amazon[0] = amazon[0].apply(lambda x: [lmt.lemmatize(y) for y in x])\n",
    "yelp[0] = yelp[0].apply(lambda x: [lmt.lemmatize(y) for y in x])\n",
    "imdb[0] = imdb[0].apply(lambda x: [lmt.lemmatize(y) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 1c: Splitting trainting and testing data\n",
    "training = pd.concat([amazon[0:800], imdb[0:800], yelp[0:800]], axis=0)\n",
    "testing = pd.concat([amazon[800:1000], imdb[800:1000], yelp[800:1000]], axis=0)\n",
    "\n",
    "training = training.reset_index(drop=True)\n",
    "testing = testing.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ## 1d: Bag of Words\n",
    "# #list1 = training[0]\n",
    "\n",
    "# asd = []\n",
    "# for i in range(0, len(list1)):\n",
    "#    a = dict((el,1) for el in list1[i])\n",
    "#    asd.append(a)\n",
    "\n",
    "# X_train = pd.DataFrame(asd)\n",
    "# X_train = X_train.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2400"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 1d: Bag of Words\n",
    "\n",
    "list_tr1 = list(training[0])\n",
    "len(list_tr1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4220"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_tr2 = [val for sublist in list_tr1 for val in sublist]\n",
    "unique_tr = set(list_tr2)\n",
    "len(unique_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_ts1 = list(testing[0])\n",
    "len(list_ts1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1766"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_ts2 = [val for sublist in list_ts1 for val in sublist]\n",
    "unique_ts = set(list_ts2)\n",
    "len(unique_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3098"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = unique_tr - unique_ts\n",
    "len(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "644"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = unique_ts - unique_tr\n",
    "len(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_all = list_tr1 + list_ts1\n",
    "len(list_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4864"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_all2 = [val for sublist in list_all for val in sublist]\n",
    "unique_all = set(list_all2)\n",
    "len(unique_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "asd = []\n",
    "for i in range(0, len(list_all)):\n",
    "    a = dict(Counter(list_all[i]))\n",
    "    asd.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 4864)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.DataFrame(asd)\n",
    "X = X.fillna(0)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 4220)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.drop(v,inplace=True,axis=1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Where are the numbers?\n",
    "x = list(X.columns.values)[0:70]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get rid of all the numbers - delete columns 0:70\n",
    "\n",
    "X_new = X[X.columns[70:4216]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aailiyah</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abhor</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>abound</th>\n",
       "      <th>abroad</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutel</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>...</th>\n",
       "      <th>youre</th>\n",
       "      <th>youthful</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yucky</th>\n",
       "      <th>yukon</th>\n",
       "      <th>yum</th>\n",
       "      <th>yummy</th>\n",
       "      <th>yun</th>\n",
       "      <th>z</th>\n",
       "      <th>zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 4146 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aailiyah  abandoned  abhor  ability  able  abound  abroad  absolute  \\\n",
       "0       0.0        0.0    0.0      0.0   0.0     0.0     0.0       0.0   \n",
       "1       0.0        0.0    0.0      0.0   0.0     0.0     0.0       0.0   \n",
       "\n",
       "   absolutel  absolutely  ...   youre  youthful  youtube  yucky  yukon  yum  \\\n",
       "0        0.0         0.0  ...     0.0       0.0      0.0    0.0    0.0  0.0   \n",
       "1        0.0         0.0  ...     0.0       0.0      0.0    0.0    0.0  0.0   \n",
       "\n",
       "   yummy  yun    z  zero  \n",
       "0    0.0  0.0  0.0   0.0  \n",
       "1    0.0  0.0  0.0   0.0  \n",
       "\n",
       "[2 rows x 4146 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_new[0:2400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = X_new[2400:3000]\n",
    "X_test = X_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1e: Standardize \n",
    "norm = (X_train.ix[0] - X_train.ix[0].mean(axis=0))/ X_train.ix[0].var(axis=0)\n",
    "for i in np.arange(1,2400):\n",
    "    x= (X_train.ix[i] - X_train.ix[i].mean(axis=0))/ X_train.ix[i].var(axis=0)\n",
    "    norm = pd.concat((norm, x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#testing\n",
    "normt = (X_test.ix[0] - X_test.ix[0].mean(axis=0))/ X_test.ix[0].var(axis=0)\n",
    "for i in np.arange(1,600):\n",
    "    x= (X_test.ix[i] - X_test.ix[i].mean(axis=0))/ X_test.ix[i].var(axis=0)\n",
    "    normt = pd.concat((normt, x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_norm = norm.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_norm = normt.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 1f: K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_norm = X_train_norm.fillna(value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test_norm = X_test_norm.fillna(value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = X_train_norm\n",
    "\n",
    "o = np.random.choice(range(data.shape[0]),2,replace=False)\n",
    "centers = data.ix[o,:]\n",
    "c1 = centers.ix[[o[0]]]\n",
    "c2 = centers.ix[[o[1]]]\n",
    "\n",
    "iteration = 0\n",
    "#repeat n times\n",
    "while iteration < 10: \n",
    "    iteration=iteration+1\n",
    "\n",
    "    #inialize variables \n",
    "    group1 = []\n",
    "    group2 = []\n",
    "    d1 = []\n",
    "    d2 = []\n",
    "    #get a list of distances -> d1 is a list of corresponding distances to c1\n",
    "    #d2 is a list of corresponding distances to c2\n",
    "    for i in range(data.shape[0]):\n",
    "        x1 = euclidean_distances(c1,data.ix[i,:].reshape(1,-1))\n",
    "        d1 = d1 + [x1[0][0]] \n",
    "        x2 = euclidean_distances(c2,data.ix[i,:].reshape(1,-1))\n",
    "        d2 = d2 + [x2[0][0]] \n",
    "\n",
    "    d = [d1,d2]\n",
    "    mins = np.argmin(d, axis = 0) #find the index of the smaller distance for each column -- if 0 then closer to c1\n",
    "    for j in range(data.shape[0]):\n",
    "        if mins[j] == 0:  #if the value in d1 was smaller for i, let i be in group1\n",
    "            group1.append(j)\n",
    "        if mins[j] == 1:  #if the value in d2 was smaller for i, let i be in group2\n",
    "            group2.append(j)\n",
    "\n",
    "    #update the centers with the average of the group \n",
    "    g1 = data.ix[group1,:]\n",
    "    g1_mean = g1.mean(axis=0)\n",
    "    c1 = pd.DataFrame(g1_mean.reshape(1,-1))\n",
    "    g2 = data.ix[group2,:]\n",
    "    g2_mean = g2.mean(axis=0)\n",
    "    c2 = pd.DataFrame(g2_mean.reshape(1,-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 285, 1282])"
      ]
     },
     "execution_count": 752,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Great phone!.\n",
       "1                1\n",
       "Name: 285, dtype: object"
      ]
     },
     "execution_count": 756,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find what the reviews are \n",
    "amazon = pd.read_table('/Users/leighajarett/Downloads/sent/amazon_cells_labelled.txt', sep ='\\t', header = None)\n",
    "amazon.ix[285,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Here's where the movie really pi**ed me off.  \n",
       "1                                                 0\n",
       "Name: 282, dtype: object"
      ]
     },
     "execution_count": 759,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb = pd.read_csv('/Users/leighajarett/Desktop/imdb.csv', skiprows = 1, header = None)\n",
    "imdb.ix[282,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    101\n",
       "0      2\n",
       "Name: 1, dtype: int64"
      ]
     },
     "execution_count": 743,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting labels for group1\n",
    "train_labs = training.ix[group1,1]\n",
    "train_labs.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1151\n",
       "1    1146\n",
       "Name: 1, dtype: int64"
      ]
     },
     "execution_count": 744,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting labels for group2\n",
    "train_labs2 = training.ix[group2,1]\n",
    "train_labs2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test using sklearn kmeans\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit_predict(X_train_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 115, 2285])"
      ]
     },
     "execution_count": 761,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_train = training.ix[:,1] #training labels\n",
    "Y_test = testing.ix[:,1] #testing labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1g: Logistic regression model\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cls = linear_model.LogisticRegression()\n",
    "fit = cls.fit(X_train_norm, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predic_labels = fit.predict(X_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accuracy(predic_label,actual_label): \n",
    "    good = 0\n",
    "    for i in range(0,len(predic_label)):\n",
    "        if predic_label[i] == actual_label[i]:\n",
    "            good =good+1  #if labels match then good gets a 1, if not good gets a 0\n",
    "        else:\n",
    "            good =good+0\n",
    "    return(good/len(predic_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7066666666666667"
      ]
     },
     "execution_count": 768,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = accuracy(predic_labels, Y_test)\n",
    "acc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[255,  92],\n",
       "       [ 84, 169]])"
      ]
     },
     "execution_count": 769,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confusion matrix \n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(Y_test, predic_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1569, 2114, 2390,  912, 1223, 1301,  110,  233, 2115, 2059])"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect weight vector\n",
    "w = fit.coef_\n",
    "idx = (-w).argsort()[:10]\n",
    "idx[0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great love nice delicious excellent fantastic amazing awesome loved liked\n"
     ]
    }
   ],
   "source": [
    "nams = list(data_fill.columns.values)\n",
    "print(nams[1569], nams[2114], nams[2390], nams[912], nams[1223], nams[1301], nams[110], nams[233], nams[2115], nams[2059])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ngram model, n=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#start with original data\n",
    "#take out punctuation and make all lowercase\n",
    "\n",
    "amazon = pd.read_table('/Users/leighajarett/Downloads/sent/amazon_cells_labelled.txt', sep ='\\t', header = None)\n",
    "yelp = pd.read_table('/Users/leighajarett/Downloads/sent/yelp_labelled.txt', sep ='\\t', header = None)\n",
    "imdb = pd.read_csv('/Users/leighajarett/Desktop/imdb.csv', skiprows = 1, header = None)\n",
    "\n",
    "amazon = amazon.apply(lambda x: x.astype(str).str.lower())\n",
    "imdb = imdb.apply(lambda x: x.astype(str).str.lower())\n",
    "yelp = yelp.apply(lambda x: x.astype(str).str.lower())\n",
    "\n",
    "strings_a = amazon[0].apply(remove_punc)\n",
    "labels_a = amazon[1]\n",
    "amazon = pd.concat([strings_a, labels_a], axis=1)\n",
    "\n",
    "strings_i = imdb[0].apply(remove_punc)\n",
    "labels_i = imdb[1]\n",
    "imdb = pd.concat([strings_i, labels_i], axis=1)\n",
    "\n",
    "strings_y = yelp[0].apply(remove_punc)\n",
    "labels_y = yelp[1]\n",
    "yelp = pd.concat([strings_y, labels_y], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# separate into training and testing\n",
    "training_n = pd.concat([amazon[0:800], imdb[0:800], yelp[0:800]], axis=0)\n",
    "testing_n = pd.concat([amazon[800:1000], imdb[800:1000], yelp[800:1000]], axis=0)\n",
    "\n",
    "training_n = training_n.reset_index(drop=True)\n",
    "testing_n = testing_n.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get ngrams for a string\n",
    "def ngrams(input, n):\n",
    "    input = input.split(' ')\n",
    "    output = []\n",
    "    for i in range(len(input)-n+1):\n",
    "        output.append(input[i:i+n])\n",
    "    return output\n",
    "#http://stackoverflow.com/questions/13423919/computing-n-grams-using-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make dictionary of ngrams for training \n",
    "dict_n= []\n",
    "for i in range(2400):\n",
    "    n = ngrams(training_n[0][i],2)\n",
    "    dict_n = dict_n + n  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_ngrams = [list(x) for x in set(tuple(x) for x in dict_n)]\n",
    "#http://stackoverflow.com/questions/3724551/python-uniqueness-for-list-of-lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get ngrams for all data\n",
    "amazon[0] = amazon[0].apply(lambda x: ngrams(x,2))\n",
    "imdb[0] = imdb[0].apply(lambda x: ngrams(x,2))\n",
    "yelp[0] = yelp[0].apply(lambda x: ngrams(x,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#separate the ngrams into testing and training\n",
    "training_ngram = pd.concat([amazon[0:800], imdb[0:800], yelp[0:800]], axis=0)\n",
    "testing_ngram = pd.concat([amazon[800:1000], imdb[800:1000], yelp[800:1000]], axis=0)\n",
    "\n",
    "training_ngram = training_ngram.reset_index(drop=True)\n",
    "testing_ngram = testing_ngram.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make bag of ngrams\n",
    "ntrain_bagwords = np.zeros((2400,len(unique_ngrams)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(unique_ngrams)):\n",
    "    for j in range(2400):\n",
    "        if unique_ngrams[i] in training_ngram[0][j]:\n",
    "            ntrain_bagwords[j][i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(ntrain_bagwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_names = unique_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make bag of words for testing\n",
    "ntest_bagwords = np.zeros((600,len(unique_ngrams)))\n",
    "\n",
    "for i in range(len(unique_ngrams)):\n",
    "    for j in range(600):\n",
    "        if unique_ngrams[i] in testing_ngram[0][j]:\n",
    "            ntest_bagwords[j][i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame(ntest_bagwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Standardize the testing and training\n",
    "Xng_train_norm = (df.ix[0] - df.ix[0].mean(axis=0))/ df.ix[0].var(axis=0)\n",
    "for i in np.arange(1,2400):\n",
    "    x= (df.ix[i] - df.ix[i].mean(axis=0))/ df.ix[i].var(axis=0)\n",
    "    Xng_train_norm = pd.concat((Xng_train_norm, x), axis=1)\n",
    "    \n",
    "Xng_test_norm = (df_test.ix[0] - df_test.ix[0].mean(axis=0))/ df_test.ix[0].var(axis=0)\n",
    "for i in np.arange(1,600):\n",
    "    x= (df_test.ix[i] - df_test.ix[i].mean(axis=0))/ df_test.ix[i].var(axis=0)\n",
    "    Xng_test_norm = pd.concat((Xng_test_norm, x), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xng_test_norm = norm_ntest.T\n",
    "Xng_train_norm = norm_ntr.T\n",
    "\n",
    "Xng_test_norm = Xng_test_norm.fillna(value = 0)\n",
    "Xng_train_norm = Xng_train_norm.fillna(value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#K-means cluster on ngrams training bag of words\n",
    "\n",
    "data = Xng_train_norm\n",
    "\n",
    "o = np.random.choice(range(data.shape[0]),2,replace=False)\n",
    "centers = data.ix[o,:]\n",
    "c1 = centers.ix[[o[0]]]\n",
    "c2 = centers.ix[[o[1]]]\n",
    "\n",
    "iteration = 0\n",
    "#repeat n times\n",
    "while iteration < 10: \n",
    "    iteration=iteration+1\n",
    "\n",
    "    #inialize variables \n",
    "    group1 = []\n",
    "    group2 = []\n",
    "    d1 = []\n",
    "    d2 = []\n",
    "    #get a list of distances -> d1 is a list of corresponding distances to c1\n",
    "    #d2 is a list of corresponding distances to c2\n",
    "    for i in range(data.shape[0]):\n",
    "        x1 = euclidean_distances(c1,data.ix[i,:].reshape(1,-1))\n",
    "        d1 = d1 + [x1[0][0]] \n",
    "        x2 = euclidean_distances(c2,data.ix[i,:].reshape(1,-1))\n",
    "        d2 = d2 + [x2[0][0]] \n",
    "\n",
    "    d = [d1,d2]\n",
    "    mins = np.argmin(d, axis = 0) #find the index of the smaller distance for each column -- if 0 then closer to c1\n",
    "    for j in range(data.shape[0]):\n",
    "        if mins[j] == 0:  #if the value in d1 was smaller for i, let i be in group1\n",
    "            group1.append(j)\n",
    "        if mins[j] == 1:  #if the value in d2 was smaller for i, let i be in group2\n",
    "            group2.append(j)\n",
    "\n",
    "    #update the centers with the average of the group \n",
    "    g1 = data.ix[group1,:]\n",
    "    g1_mean = g1.mean(axis=0)\n",
    "    c1 = pd.DataFrame(g1_mean.reshape(1,-1))\n",
    "    g2 = data.ix[group2,:]\n",
    "    g2_mean = g2.mean(axis=0)\n",
    "    c2 = pd.DataFrame(g2_mean.reshape(1,-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1071\n",
       "0     938\n",
       "Name: 1, dtype: int64"
      ]
     },
     "execution_count": 774,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check out results of KMeans\n",
    "\n",
    "#getting labels for group1\n",
    "train_labs_n1 = training.ix[group1,1]\n",
    "train_labs_n1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    215\n",
       "1    176\n",
       "Name: 1, dtype: int64"
      ]
     },
     "execution_count": 775,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting labels for group2\n",
    "train_labs_n2 = training.ix[group2,1]\n",
    "train_labs_n2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1724,  868])"
      ]
     },
     "execution_count": 780,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get centers\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this movie totally grates on my nerves  '"
      ]
     },
     "execution_count": 790,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_n.ix[868,:][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'we thought youd have to venture further away to get good sushi but this place really hit the spot that night'"
      ]
     },
     "execution_count": 789,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_n.ix[1724,:][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Repeat Logistic Regression\n",
    "cls = linear_model.LogisticRegression()\n",
    "fit = cls.fit(Xng_train_norm, Y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predic_labels_n = fit.predict(Xng_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6966666666666667"
      ]
     },
     "execution_count": 778,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get accruacy\n",
    "acc_n = accuracy(predic_labels_n, Y_test)\n",
    "acc_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[235, 112],\n",
       "       [ 70, 183]])"
      ]
     },
     "execution_count": 779,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get confusion matrix \n",
    "confusion_matrix(Y_test, predic_labels_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5209, 10483, 14285, 14164,  4024, 13669,  9128,  9973,  2173, 10109])"
      ]
     },
     "execution_count": 599,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inspect weight vector \n",
    "w = fit.coef_\n",
    "idx = (-w).argsort()[:5]\n",
    "idx[0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['works', 'great'] ['a', 'great'] ['the', 'best'] ['very', 'good'] ['love', 'this'] ['great', 'phone'] ['is', 'good'] ['love', 'the'] ['well', ''] ['i', 'liked']\n"
     ]
    }
   ],
   "source": [
    "print(unique_ngrams[5209], unique_ngrams[10483], unique_ngrams[14285], unique_ngrams[14164], unique_ngrams[4024], unique_ngrams[13669], unique_ngrams[9128], unique_ngrams[9973], unique_ngrams[2173], unique_ngrams[10109])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PCA \n",
    "#http://sebastianraschka.com/Articles/2014_pca_step_by_step.html\n",
    "\n",
    "#working with normalized feature vectors X_test_fill, Y_test, X_train_norm, Y_train\n",
    "#combine testing and training to perform PCA on all data\n",
    "all_data = pd.concat((data_fill,X_test_fill))\n",
    "all_data_labels = pd.concat((Y_train,Y_test))\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#compute mean training feature vector \n",
    "#train_mean = X_train_norm.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4146, 4146)"
      ]
     },
     "execution_count": 639,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#computing covariance matrix\n",
    "cov_mat = np.cov(all_data.T)\n",
    "cov_mat.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute eigenvectors and corresponding eigenvalues\n",
    "eig_val, eig_vec = np.linalg.eig(cov_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sorting eigenvectors by decreasing eigenvalues\n",
    "\n",
    "#pair eigenvectors and values\n",
    "eig_pairs = [(np.abs(eig_val[i]), eig_vec[:,i]) for i in range(len(eig_val))]\n",
    "\n",
    "#sort\n",
    "eig_pairs.sort(key=lambda x: x[0], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104797.32244680138,\n",
       " array([  1.97396706e-04+0.j,   2.92788586e-05+0.j,  -2.85953675e-04+0.j,\n",
       "        ...,   2.92788586e-05+0.j,   6.73659117e-05+0.j,\n",
       "          6.20491522e-04+0.j]))"
      ]
     },
     "execution_count": 642,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eig_pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reduce to 10-d\n",
    "#make a matrix of eigenvalues along the columns, so will have 10 columns \n",
    "matrix_w_10 = eig_pairs[0][1].reshape(4146,1)\n",
    "for i in np.arange(1,10):\n",
    "    matrix_w_10 = np.hstack((matrix_w_10,eig_pairs[i][1].reshape(4146,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4146, 10)"
      ]
     },
     "execution_count": 644,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_w_10.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#transform samples onto new subspace\n",
    "transformed_10 = matrix_w_10.T.dot(all_data.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 3000)"
      ]
     },
     "execution_count": 650,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_10.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trans_10_data = np.real(transformed_10.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Reduce to 50-d\n",
    "matrix_w_50 = eig_pairs[0][1].reshape(4146,1)\n",
    "for i in np.arange(1,50):\n",
    "    matrix_w_50 = np.hstack((matrix_w_50,eig_pairs[i][1].reshape(4146,1)))\n",
    "    \n",
    "transformed_50 = matrix_w_50.T.dot(all_data.T)\n",
    "trans_50_data = np.real(transformed_50.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Reduce to 100-d\n",
    "matrix_w_100 = eig_pairs[0][1].reshape(4146,1)\n",
    "for i in np.arange(1,100):\n",
    "    matrix_w_100 = np.hstack((matrix_w_100,eig_pairs[i][1].reshape(4146,1)))\n",
    "    \n",
    "transformed_100 = matrix_w_100.T.dot(all_data.T)\n",
    "trans_100_data = np.real(transformed_100.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Repeat Kmeans for 10D\n",
    "\n",
    "data = pd.DataFrame(trans_10_data[0:2400]) #perform only on training \n",
    "\n",
    "o = np.random.choice(range(data.shape[0]),2,replace=False)\n",
    "centers = data.ix[o,:]\n",
    "c1 = centers.ix[[o[0]]]\n",
    "c2 = centers.ix[[o[1]]]\n",
    "\n",
    "iteration = 0\n",
    "#repeat n times\n",
    "while iteration < 10: \n",
    "    iteration=iteration+1\n",
    "\n",
    "    #inialize variables \n",
    "    group1 = []\n",
    "    group2 = []\n",
    "    d1 = []\n",
    "    d2 = []\n",
    "    #get a list of distances -> d1 is a list of corresponding distances to c1\n",
    "    #d2 is a list of corresponding distances to c2\n",
    "    for i in range(data.shape[0]):\n",
    "        x1 = euclidean_distances(c1,data.ix[i,:].reshape(1,-1))\n",
    "        d1 = d1 + [x1[0][0]] \n",
    "        x2 = euclidean_distances(c2,data.ix[i,:].reshape(1,-1))\n",
    "        d2 = d2 + [x2[0][0]] \n",
    "\n",
    "    d = [d1,d2]\n",
    "    mins = np.argmin(d, axis = 0) #find the index of the smaller distance for each column -- if 0 then closer to c1\n",
    "    for j in range(data.shape[0]):\n",
    "        if mins[j] == 0:  #if the value in d1 was smaller for i, let i be in group1\n",
    "            group1.append(j)\n",
    "        if mins[j] == 1:  #if the value in d2 was smaller for i, let i be in group2\n",
    "            group2.append(j)\n",
    "\n",
    "    #update the centers with the average of the group \n",
    "    g1 = data.ix[group1,:]\n",
    "    g1_mean = g1.mean(axis=0)\n",
    "    c1 = pd.DataFrame(g1_mean.reshape(1,-1))\n",
    "    g2 = data.ix[group2,:]\n",
    "    g2_mean = g2.mean(axis=0)\n",
    "    c2 = pd.DataFrame(g2_mean.reshape(1,-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    93\n",
       "0    21\n",
       "Name: 1, dtype: int64"
      ]
     },
     "execution_count": 675,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#look at results\n",
    "#getting labels for group1\n",
    "train_labs_pca_10 = training.ix[group1,1]\n",
    "train_labs_pca_10.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1154\n",
       "0    1132\n",
       "Name: 1, dtype: int64"
      ]
     },
     "execution_count": 676,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting labels for group2\n",
    "train_labs_pca_10 = training.ix[group2,1]\n",
    "train_labs_pca_10.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Repeat Logistic Regresstion for 10D\n",
    "cls = linear_model.LogisticRegression()\n",
    "fit = cls.fit(data, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predic_labels_pca_10 = fit.predict(pd.DataFrame(trans_10_data[2400:3000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.585"
      ]
     },
     "execution_count": 679,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_pca_10 = accuracy(predic_labels_pca_10, Y_test)\n",
    "acc_pca_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[257,  90],\n",
       "       [159,  94]])"
      ]
     },
     "execution_count": 680,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(Y_test, predic_labels_pca_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Repeat Kmeans for 50D\n",
    "\n",
    "data = pd.DataFrame(trans_50_data[0:2400]) #perform only on training \n",
    "\n",
    "o = np.random.choice(range(data.shape[0]),2,replace=False)\n",
    "centers = data.ix[o,:]\n",
    "c1 = centers.ix[[o[0]]]\n",
    "c2 = centers.ix[[o[1]]]\n",
    "\n",
    "iteration = 0\n",
    "#repeat n times\n",
    "while iteration < 10: \n",
    "    iteration=iteration+1\n",
    "\n",
    "    #inialize variables \n",
    "    group1 = []\n",
    "    group2 = []\n",
    "    d1 = []\n",
    "    d2 = []\n",
    "    #get a list of distances -> d1 is a list of corresponding distances to c1\n",
    "    #d2 is a list of corresponding distances to c2\n",
    "    for i in range(data.shape[0]):\n",
    "        x1 = euclidean_distances(c1,data.ix[i,:].reshape(1,-1))\n",
    "        d1 = d1 + [x1[0][0]] \n",
    "        x2 = euclidean_distances(c2,data.ix[i,:].reshape(1,-1))\n",
    "        d2 = d2 + [x2[0][0]] \n",
    "\n",
    "    d = [d1,d2]\n",
    "    mins = np.argmin(d, axis = 0) #find the index of the smaller distance for each column -- if 0 then closer to c1\n",
    "    for j in range(data.shape[0]):\n",
    "        if mins[j] == 0:  #if the value in d1 was smaller for i, let i be in group1\n",
    "            group1.append(j)\n",
    "        if mins[j] == 1:  #if the value in d2 was smaller for i, let i be in group2\n",
    "            group2.append(j)\n",
    "\n",
    "    #update the centers with the average of the group \n",
    "    g1 = data.ix[group1,:]\n",
    "    g1_mean = g1.mean(axis=0)\n",
    "    c1 = pd.DataFrame(g1_mean.reshape(1,-1))\n",
    "    g2 = data.ix[group2,:]\n",
    "    g2_mean = g2.mean(axis=0)\n",
    "    c2 = pd.DataFrame(g2_mean.reshape(1,-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    102\n",
       "1     71\n",
       "Name: 1, dtype: int64"
      ]
     },
     "execution_count": 685,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#look at results\n",
    "#getting labels for group1\n",
    "train_labs_pca_50 = training.ix[group1,1]\n",
    "train_labs_pca_50.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1176\n",
       "0    1051\n",
       "Name: 1, dtype: int64"
      ]
     },
     "execution_count": 686,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting labels for group2\n",
    "train_labs_pca_50 = training.ix[group2,1]\n",
    "train_labs_pca_50.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Repeat Logistic Regresstion for 50D\n",
    "cls = linear_model.LogisticRegression()\n",
    "fit = cls.fit(data, Y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predic_labels_pca_50 = fit.predict(pd.DataFrame(trans_50_data[2400:3000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6616666666666666"
      ]
     },
     "execution_count": 690,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_pca_50 = accuracy(predic_labels_pca_50, Y_test)\n",
    "acc_pca_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[252,  95],\n",
       "       [108, 145]])"
      ]
     },
     "execution_count": 691,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(Y_test, predic_labels_pca_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Repeat Kmeans for 100D\n",
    "\n",
    "data = pd.DataFrame(trans_100_data[0:2400]) #perform only on training \n",
    "\n",
    "o = np.random.choice(range(data.shape[0]),2,replace=False)\n",
    "centers = data.ix[o,:]\n",
    "c1 = centers.ix[[o[0]]]\n",
    "c2 = centers.ix[[o[1]]]\n",
    "\n",
    "iteration = 0\n",
    "#repeat n times\n",
    "while iteration < 10: \n",
    "    iteration=iteration+1\n",
    "\n",
    "    #inialize variables \n",
    "    group1 = []\n",
    "    group2 = []\n",
    "    d1 = []\n",
    "    d2 = []\n",
    "    #get a list of distances -> d1 is a list of corresponding distances to c1\n",
    "    #d2 is a list of corresponding distances to c2\n",
    "    for i in range(data.shape[0]):\n",
    "        x1 = euclidean_distances(c1,data.ix[i,:].reshape(1,-1))\n",
    "        d1 = d1 + [x1[0][0]] \n",
    "        x2 = euclidean_distances(c2,data.ix[i,:].reshape(1,-1))\n",
    "        d2 = d2 + [x2[0][0]] \n",
    "\n",
    "    d = [d1,d2]\n",
    "    mins = np.argmin(d, axis = 0) #find the index of the smaller distance for each column -- if 0 then closer to c1\n",
    "    for j in range(data.shape[0]):\n",
    "        if mins[j] == 0:  #if the value in d1 was smaller for i, let i be in group1\n",
    "            group1.append(j)\n",
    "        if mins[j] == 1:  #if the value in d2 was smaller for i, let i be in group2\n",
    "            group2.append(j)\n",
    "\n",
    "    #update the centers with the average of the group \n",
    "    g1 = data.ix[group1,:]\n",
    "    g1_mean = g1.mean(axis=0)\n",
    "    c1 = pd.DataFrame(g1_mean.reshape(1,-1))\n",
    "    g2 = data.ix[group2,:]\n",
    "    g2_mean = g2.mean(axis=0)\n",
    "    c2 = pd.DataFrame(g2_mean.reshape(1,-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    15\n",
       "1    12\n",
       "Name: 1, dtype: int64"
      ]
     },
     "execution_count": 693,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#look at results\n",
    "#getting labels for group1\n",
    "train_labs_pca_100 = training.ix[group1,1]\n",
    "train_labs_pca_100.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1235\n",
       "0    1138\n",
       "Name: 1, dtype: int64"
      ]
     },
     "execution_count": 694,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting labels for group2\n",
    "train_labs_pca_100 = training.ix[group2,1]\n",
    "train_labs_pca_100.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Repeat Logistic Regresstion for 100D\n",
    "cls = linear_model.LogisticRegression()\n",
    "fit = cls.fit(data, Y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predic_labels_pca_100 = fit.predict(pd.DataFrame(trans_100_data[2400:3000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7133333333333334"
      ]
     },
     "execution_count": 697,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_pca_100 = accuracy(predic_labels_pca_100, Y_test)\n",
    "acc_pca_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[263,  84],\n",
       "       [ 88, 165]])"
      ]
     },
     "execution_count": 698,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(Y_test, predic_labels_pca_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
